{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from app_utils import AppUtils\n",
    "from app_utils.app_alerts import AlertService\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING INITIALIZATION AND CONFIGURATION\n",
    "\n",
    "# Create a custom configuration for testing\n",
    "test_config = {\n",
    "    \"percentile_categories\": {\n",
    "        \"SB\": 5.0,  # Customize thresholds for testing\n",
    "        \"B\": 20.0,\n",
    "        \"N\": 80.0,\n",
    "        \"G\": 95.0,\n",
    "        \"SG\": 100\n",
    "    },\n",
    "    \"feature_config\": {\n",
    "        \"test_feature\": {\n",
    "            \"description\": \"Test Feature\",\n",
    "            \"importance\": \"high\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentile 1.0 -> Category: SB (Significantly Below Average)\n",
      "Percentile 10.0 -> Category: B (Below Average)\n",
      "Percentile 50.0 -> Category: N (Average)\n",
      "Percentile 85.0 -> Category: G (Above Average)\n",
      "Percentile 98.0 -> Category: SG (Significantly Above Average)\n",
      "\n",
      "Configuration:\n",
      "Percentile Categories: {'SB': 5.0, 'B': 20.0, 'N': 80.0, 'G': 95.0, 'SG': 100}\n",
      "Feature Config: {'test_feature': {'description': 'Test Feature', 'importance': 'high'}}\n",
      "\n",
      "AppUtils set: True\n",
      "Analyzers validated: False\n"
     ]
    }
   ],
   "source": [
    "alert_service = AlertService(config=test_config)\n",
    "\n",
    "# Test percentile mapping\n",
    "test_percentiles = [1.0, 10.0, 50.0, 85.0, 98.0]\n",
    "for p in test_percentiles:\n",
    "    category = alert_service.map_percentile_to_category(p)\n",
    "    description = alert_service.get_category_description(category)\n",
    "    print(f\"Percentile {p:.1f} -> Category: {category} ({description})\")\n",
    "\n",
    "# Verify configuration was applied correctly\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"Percentile Categories: {alert_service.config['percentile_categories']}\")\n",
    "print(f\"Feature Config: {alert_service.config['feature_config']}\")\n",
    "\n",
    "# Verify AppUtils integration\n",
    "utils = AppUtils()\n",
    "alert_service.set_app_utils(utils)\n",
    "print(f\"\\nAppUtils set: {alert_service.app_utils is not None}\")\n",
    "\n",
    "# Test validation of analyzers (should be False since we haven't initialized them yet)\n",
    "print(f\"Analyzers validated: {alert_service._validate_analyzers()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for testing\n",
    "def create_sample_session_data():\n",
    "    \"\"\"Create sample session data for testing\"\"\"\n",
    "    # Create dates for the last 10 days\n",
    "    dates = [datetime.now() - timedelta(days=i) for i in range(10)]\n",
    "    \n",
    "    # Create sample data for 3 subjects\n",
    "    subjects = ['sub001', 'sub002', 'sub003']\n",
    "    \n",
    "    # Create dataframe rows\n",
    "    rows = []\n",
    "    for subject in subjects:\n",
    "        for i, date in enumerate(dates):\n",
    "            # Sample feature values\n",
    "            trials = 100 - i * 5  # Decreasing trials (100, 95, 90, ...)\n",
    "            errors = i * 2        # Increasing errors (0, 2, 4, ...)\n",
    "            \n",
    "            # Add some variability for subject 2\n",
    "            if subject == 'sub002':\n",
    "                trials = trials * 0.8  # 20% fewer trials\n",
    "                errors = errors * 1.5  # 50% more errors\n",
    "            \n",
    "            # Add each session\n",
    "            rows.append({\n",
    "                'subject_id': subject,\n",
    "                'session_date': date,\n",
    "                'session': f\"session_{i+1}\",\n",
    "                'total_trials': trials,\n",
    "                'error_count': errors\n",
    "            })\n",
    "    \n",
    "    # Create dataframe\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample data with 30 sessions for 3 subjects\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize AppUtils and AlertService\n",
    "utils = AppUtils()\n",
    "alert_service = AlertService(app_utils=utils)\n",
    "\n",
    "# Create sample data\n",
    "sample_data = create_sample_session_data()\n",
    "print(f\"Created sample data with {len(sample_data)} sessions for {len(sample_data['subject_id'].unique())} subjects\")\n",
    "\n",
    "# Create feature thresholds for testing\n",
    "feature_thresholds = {\n",
    "    'total_trials': {\n",
    "        'lower': 80  # Alert if trials are below 80\n",
    "    },\n",
    "    'error_count': {\n",
    "        'upper': 10  # Alert if errors are above 10\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up mock analyzers...\n"
     ]
    }
   ],
   "source": [
    "# Skip the real initialization and mock directly\n",
    "print(\"Setting up mock analyzers...\")\n",
    "\n",
    "class MockThresholdAnalyzer:\n",
    "    def __init__(self, sample_data, thresholds):\n",
    "        self.sample_data = sample_data\n",
    "        self.thresholds = thresholds\n",
    "        \n",
    "        # Add threshold crossing columns\n",
    "        self.threshold_data = sample_data.copy()\n",
    "        for feature, config in thresholds.items():\n",
    "            if 'lower' in config:\n",
    "                self.threshold_data[f\"{feature}_above_lower\"] = self.threshold_data[feature] >= config['lower']\n",
    "            if 'upper' in config:\n",
    "                self.threshold_data[f\"{feature}_below_upper\"] = self.threshold_data[feature] <= config['upper']\n",
    "        \n",
    "        # Create summary data\n",
    "        summary_data = []\n",
    "        for subject in sample_data['subject_id'].unique():\n",
    "            subject_df = self.threshold_data[self.threshold_data['subject_id'] == subject]\n",
    "            summary = {'subject_id': subject, 'total_sessions': len(subject_df)}\n",
    "            \n",
    "            # Process each threshold\n",
    "            for feature, config in thresholds.items():\n",
    "                if 'lower' in config:\n",
    "                    col = f\"{feature}_above_lower\"\n",
    "                    passed = subject_df[col].sum()\n",
    "                    summary[f\"{col}_count\"] = passed\n",
    "                    summary[f\"{col}_percent\"] = (passed / len(subject_df)) * 100\n",
    "                    if passed > 0:\n",
    "                        summary[f\"{col}_first_date\"] = subject_df[subject_df[col]]['session_date'].min()\n",
    "                \n",
    "                if 'upper' in config:\n",
    "                    col = f\"{feature}_below_upper\"\n",
    "                    passed = subject_df[col].sum()\n",
    "                    summary[f\"{col}_count\"] = passed\n",
    "                    summary[f\"{col}_percent\"] = (passed / len(subject_df)) * 100\n",
    "                    if passed > 0:\n",
    "                        summary[f\"{col}_first_date\"] = subject_df[subject_df[col]]['session_date'].min()\n",
    "            \n",
    "            summary_data.append(summary)\n",
    "        \n",
    "        self.summary_data = pd.DataFrame(summary_data)\n",
    "\n",
    "class MockQuantileAnalyzer:\n",
    "    \"\"\"Empty mock for the quantile analyzer\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "# Create mock analyzers and attach to utils\n",
    "mock_threshold = MockThresholdAnalyzer(sample_data, feature_thresholds)\n",
    "mock_quantile = MockQuantileAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.threshold_analyzer = mock_threshold\n",
    "utils.quantile_analyzer = mock_quantile\n",
    "\n",
    "# Add mock methods with the EXACT method names from your alert_service.py\n",
    "utils.get_threshold_crossing = lambda subject_ids=None, start_date=None, end_date=None: mock_threshold.threshold_data.copy()\n",
    "utils.get_subject_threshold_summary = lambda subject_ids=None: mock_threshold.summary_data.copy()\n",
    "\n",
    "# Override the validation method for testing\n",
    "original_validate = alert_service._validate_analyzers\n",
    "alert_service._validate_analyzers = lambda: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating threshold alerts...\n",
      "Generated alerts for 3 subjects\n",
      "\n",
      "Subject: sub001\n",
      "  Feature: total_trials\n",
      "    above_lower: value=55.0 count=5\n",
      "  Feature: error_count\n",
      "    below_upper: value=18.0 count=6\n",
      "\n",
      "Subject: sub002\n",
      "  Feature: total_trials\n",
      "    above_lower: value=44.0 count=1\n",
      "  Feature: error_count\n",
      "    below_upper: value=27.0 count=4\n",
      "\n",
      "Subject: sub003\n",
      "  Feature: total_trials\n",
      "    above_lower: value=55.0 count=5\n",
      "  Feature: error_count\n",
      "    below_upper: value=18.0 count=6\n",
      "\n",
      "Subjects with any alert: ['sub001', 'sub002', 'sub003']\n",
      "Subjects with trial alerts: ['sub001', 'sub002', 'sub003']\n",
      "\n",
      "Alert summary for sub001: 2 features with alerts: total_trials,error_count (2 total conditions)\n",
      "\n",
      "Alert summary for sub002: 2 features with alerts: total_trials,error_count (2 total conditions)\n",
      "\n",
      "Alert summary for sub003: 2 features with alerts: total_trials,error_count (2 total conditions)\n"
     ]
    }
   ],
   "source": [
    "# Now try calculating alerts\n",
    "print(\"Calculating threshold alerts...\")\n",
    "alerts = alert_service.calculate_threshold_alerts()\n",
    "print(f\"Generated alerts for {len(alerts)} subjects\")\n",
    "\n",
    "# Display alerts for each subject\n",
    "for subject_id, feature_alerts in alerts.items():\n",
    "    print(f\"\\nSubject: {subject_id}\")\n",
    "    for feature, conditions in feature_alerts.items():\n",
    "        print(f\"  Feature: {feature}\")\n",
    "        for condition, details in conditions.items():\n",
    "            value_str = f\"value={details['value']}\" if details['value'] is not None else \"\"\n",
    "            count_str = f\"count={details['crossing_count']}\" if details['crossing_count'] is not None else \"\"\n",
    "            print(f\"    {condition}: {value_str} {count_str}\")\n",
    "\n",
    "# Test getting subjects with alerts\n",
    "subjects_with_alerts = alert_service.get_subjects_with_threshold_alerts()\n",
    "print(f\"\\nSubjects with any alert: {subjects_with_alerts}\")\n",
    "\n",
    "# Test getting subjects with specific feature alerts\n",
    "subjects_with_trial_alerts = alert_service.get_subjects_with_threshold_alerts(['total_trials'])\n",
    "print(f\"Subjects with trial alerts: {subjects_with_trial_alerts}\")\n",
    "\n",
    "# Test getting alert summary\n",
    "for subject_id in alerts:\n",
    "    summary = alert_service.get_threshold_alert_summary(subject_id)\n",
    "    print(f\"\\nAlert summary for {subject_id}: {summary}\")\n",
    "\n",
    "# Restore original validation method\n",
    "alert_service._validate_analyzers = original_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sample percentile data...\n",
      "Created data with 15 rows for 5 subjects\n",
      "\n",
      "Columns in sample data:\n",
      "['subject_id', 'strata', 'is_current', 'session_count', 'first_date', 'last_date', 'trials_percentile', 'trials_processed', 'errors_percentile', 'errors_processed', 'accuracy_percentile', 'accuracy_processed']\n",
      "\n",
      "Sample data row:\n",
      "{'subject_id': 'sub001', 'strata': 'Task1_ADVANCED_v3', 'is_current': True, 'session_count': 10, 'first_date': Timestamp('2025-02-22 14:29:30.742384'), 'last_date': Timestamp('2025-03-24 14:29:30.742393'), 'trials_percentile': 95.0, 'trials_processed': 9.5, 'errors_percentile': 10.0, 'errors_processed': 1.0, 'accuracy_percentile': 97.0, 'accuracy_processed': 9.7}\n"
     ]
    }
   ],
   "source": [
    "# Create sample data for percentile testing\n",
    "def create_sample_percentile_data():\n",
    "    \"\"\"Create sample data for testing percentile-based alerts\"\"\"\n",
    "    # Define strata\n",
    "    strata = ['Task1_ADVANCED_v3', 'Task1_INTERMEDIATE_v3', 'Task2_BEGINNER_v3']\n",
    "    \n",
    "    # Define features with percentile values\n",
    "    features = {\n",
    "        'trials': {\n",
    "            'sub001': {'current': 95.0, 'historical': [80.0, 60.0]},\n",
    "            'sub002': {'current': 30.0, 'historical': [20.0, 10.0]},\n",
    "            'sub003': {'current': 50.0, 'historical': [45.0, 55.0]},\n",
    "            'sub004': {'current': 99.0, 'historical': [98.0, 97.0]},\n",
    "            'sub005': {'current': 1.5, 'historical': [5.0, 10.0]}\n",
    "        },\n",
    "        'errors': {\n",
    "            'sub001': {'current': 10.0, 'historical': [15.0, 20.0]},\n",
    "            'sub002': {'current': 85.0, 'historical': [80.0, 75.0]},\n",
    "            'sub003': {'current': 40.0, 'historical': [45.0, 50.0]},\n",
    "            'sub004': {'current': 98.5, 'historical': [97.0, 96.0]},\n",
    "            'sub005': {'current': 1.0, 'historical': [2.0, 3.0]}\n",
    "        },\n",
    "        'accuracy': {\n",
    "            'sub001': {'current': 97.0, 'historical': [95.0, 90.0]},\n",
    "            'sub002': {'current': 5.0, 'historical': [10.0, 15.0]},\n",
    "            'sub003': {'current': 60.0, 'historical': [55.0, 50.0]},\n",
    "            'sub004': {'current': 92.0, 'historical': [90.0, 88.0]},\n",
    "            'sub005': {'current': 3.0, 'historical': [5.0, 8.0]}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create rows for the dataframe\n",
    "    rows = []\n",
    "    \n",
    "    # Current strata - one per subject\n",
    "    for subject_id in features['trials'].keys():\n",
    "        # Assign a strata based on subject number (simple rule for testing)\n",
    "        strata_idx = (int(subject_id[-1]) - 1) % len(strata)\n",
    "        current_strata = strata[strata_idx]\n",
    "        \n",
    "        row = {\n",
    "            'subject_id': subject_id,\n",
    "            'strata': current_strata,\n",
    "            'is_current': True,\n",
    "            'session_count': 10,\n",
    "            'first_date': datetime.now() - timedelta(days=30),\n",
    "            'last_date': datetime.now()\n",
    "        }\n",
    "        \n",
    "        # Add percentile values for each feature\n",
    "        for feature, subjects in features.items():\n",
    "            percentile = subjects[subject_id]['current']\n",
    "            row[f'{feature}_percentile'] = percentile\n",
    "            row[f'{feature}_processed'] = percentile / 10.0  # Just for testing\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    # Historical strata - vary by subject\n",
    "    for subject_id in features['trials'].keys():\n",
    "        # Use different strata for historical data\n",
    "        for i, hist_strata in enumerate(strata):\n",
    "            if hist_strata == rows[int(subject_id[-1])-1]['strata']:\n",
    "                continue  # Skip current strata\n",
    "                \n",
    "            # Create historical rows with older dates\n",
    "            row = {\n",
    "                'subject_id': subject_id,\n",
    "                'strata': hist_strata,\n",
    "                'is_current': False,\n",
    "                'session_count': 5,\n",
    "                'first_date': datetime.now() - timedelta(days=90 + i*30),\n",
    "                'last_date': datetime.now() - timedelta(days=60 + i*30)\n",
    "            }\n",
    "            \n",
    "            # Add percentile values for each feature (use historical if available)\n",
    "            for feature, subjects in features.items():\n",
    "                if i < len(subjects[subject_id]['historical']):\n",
    "                    percentile = subjects[subject_id]['historical'][i]\n",
    "                else:\n",
    "                    percentile = subjects[subject_id]['current'] - 10  # Fallback\n",
    "                    \n",
    "                row[f'{feature}_percentile'] = percentile\n",
    "                row[f'{feature}_processed'] = percentile / 10.0  # Just for testing\n",
    "            \n",
    "            rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "print(\"Creating sample percentile data...\")\n",
    "sample_percentile_data = create_sample_percentile_data()\n",
    "print(f\"Created data with {len(sample_percentile_data)} rows for {len(sample_percentile_data['subject_id'].unique())} subjects\")\n",
    "# Print column names to verify correct structure\n",
    "print(\"\\nColumns in sample data:\")\n",
    "print(sample_percentile_data.columns.tolist())\n",
    "\n",
    "# Print a sample row to verify data structure\n",
    "print(\"\\nSample data row:\")\n",
    "print(sample_percentile_data.iloc[0].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mock QuantileAnalyzer that properly implements the interface\n",
    "class MockQuantileAnalyzer:\n",
    "    def __init__(self, sample_data):\n",
    "        self.sample_data = sample_data\n",
    "    \n",
    "    def create_comprehensive_dataframe(self, include_history=False):\n",
    "        \"\"\"Return the entire sample dataset if include_history is True, otherwise only current strata\"\"\"\n",
    "        print(f\"Mock create_comprehensive_dataframe called with include_history={include_history}\")\n",
    "        if include_history:\n",
    "            result = self.sample_data.copy()\n",
    "        else:\n",
    "            result = self.sample_data[self.sample_data['is_current']].copy()\n",
    "        print(f\"Returning dataframe with {len(result)} rows\")\n",
    "        return result\n",
    "\n",
    "# Create mock threshold analyzer to satisfy validator\n",
    "class MockThresholdAnalyzer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "# Mock the analyzers and attach to utils\n",
    "utils.quantile_analyzer = MockQuantileAnalyzer(sample_percentile_data)\n",
    "utils.threshold_analyzer = MockThresholdAnalyzer()\n",
    "\n",
    "# Override the validation method to make debugging easier\n",
    "def mock_validate():\n",
    "    print(\"Validation check called, returning True\")\n",
    "    return True\n",
    "\n",
    "alert_service._validate_analyzers = mock_validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating quantile alerts...\n",
      "Validation check called, returning True\n",
      "Mock create_comprehensive_dataframe called with include_history=True\n",
      "Returning dataframe with 15 rows\n",
      "Generated quantile alerts for 5 subjects\n",
      "\n",
      "Subject: sub001\n",
      "  Current strata alerts:\n",
      "    Strata: Task1_ADVANCED_v3\n",
      "      trials: 95.0% - G (Above Average)\n",
      "      errors: 10.0% - B (Below Average)\n",
      "      accuracy: 97.0% - G (Above Average)\n",
      "\n",
      "Subject: sub002\n",
      "  Current strata alerts:\n",
      "    Strata: Task1_INTERMEDIATE_v3\n",
      "      trials: 30.0% - N (Average)\n",
      "      errors: 85.0% - N (Average)\n",
      "      accuracy: 5.0% - B (Below Average)\n",
      "\n",
      "Subject: sub003\n",
      "  Current strata alerts:\n",
      "    Strata: Task2_BEGINNER_v3\n",
      "      trials: 50.0% - N (Average)\n",
      "      errors: 40.0% - N (Average)\n",
      "      accuracy: 60.0% - N (Average)\n",
      "\n",
      "Subject: sub004\n",
      "  Current strata alerts:\n",
      "    Strata: Task1_ADVANCED_v3\n",
      "      trials: 99.0% - SG (Significantly Above Average)\n",
      "      errors: 98.5% - SG (Significantly Above Average)\n",
      "      accuracy: 92.0% - G (Above Average)\n",
      "\n",
      "Subject: sub005\n",
      "  Current strata alerts:\n",
      "    Strata: Task1_INTERMEDIATE_v3\n",
      "      trials: 1.5% - SB (Significantly Below Average)\n",
      "      errors: 1.0% - SB (Significantly Below Average)\n",
      "      accuracy: 3.0% - B (Below Average)\n"
     ]
    }
   ],
   "source": [
    "# Try to calculate quantile alerts with debugging\n",
    "print(\"\\nCalculating quantile alerts...\")\n",
    "try:\n",
    "    alerts = alert_service.calculate_quantile_alerts()\n",
    "    if alerts is None:\n",
    "        print(\"WARNING: calculate_quantile_alerts returned None\")\n",
    "    else:\n",
    "        print(f\"Generated quantile alerts for {len(alerts)} subjects\")\n",
    "        \n",
    "        # Display alerts for each subject (current strata only for brevity)\n",
    "        for subject_id, subject_alerts in alerts.items():\n",
    "            print(f\"\\nSubject: {subject_id}\")\n",
    "            print(\"  Current strata alerts:\")\n",
    "            \n",
    "            for strata, strata_alerts in subject_alerts['current'].items():\n",
    "                print(f\"    Strata: {strata}\")\n",
    "                \n",
    "                for feature, alert in strata_alerts.items():\n",
    "                    print(f\"      {feature}: {alert['percentile']:.1f}% - {alert['category']} ({alert['description']})\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating alerts: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up mock threshold analyzer...\n"
     ]
    }
   ],
   "source": [
    "## TESTING COMBINED ALERT ACCESS METHODS\n",
    "\n",
    "# Set up mock threshold analyzer\n",
    "print(\"Setting up mock threshold analyzer...\")\n",
    "threshold_data = pd.DataFrame({\n",
    "    'subject_id': ['sub001', 'sub002', 'sub003', 'sub001', 'sub002', 'sub003'],\n",
    "    'session_date': [\n",
    "        datetime.now() - timedelta(days=5),\n",
    "        datetime.now() - timedelta(days=5),\n",
    "        datetime.now() - timedelta(days=5),\n",
    "        datetime.now() - timedelta(days=1),\n",
    "        datetime.now() - timedelta(days=1),\n",
    "        datetime.now() - timedelta(days=1)\n",
    "    ],\n",
    "    'total_trials': [90, 70, 100, 85, 65, 95],\n",
    "    'error_count': [5, 15, 8, 7, 18, 9],\n",
    "    'total_trials_above_lower': [True, False, True, True, False, True],  # Above 80\n",
    "    'error_count_below_upper': [True, False, True, True, False, True]    # Below 10\n",
    "})\n",
    "\n",
    "threshold_summary = pd.DataFrame({\n",
    "    'subject_id': ['sub001', 'sub002', 'sub003'],\n",
    "    'total_sessions': [2, 2, 2],\n",
    "    'total_trials_above_lower_count': [2, 0, 2],\n",
    "    'total_trials_above_lower_percent': [100.0, 0.0, 100.0],\n",
    "    'error_count_below_upper_count': [2, 0, 2],\n",
    "    'error_count_below_upper_percent': [100.0, 0.0, 100.0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up mock quantile analyzer...\n",
      "\n",
      "Testing combined alert access methods...\n"
     ]
    }
   ],
   "source": [
    "class MockThresholdAnalyzer:\n",
    "    def __init__(self, data, summary):\n",
    "        self.data = data\n",
    "        self.summary = summary\n",
    "    \n",
    "    def get_threshold_crossings(self, subject_ids=None, start_date=None, end_date=None):\n",
    "        result = self.data.copy()\n",
    "        if subject_ids is not None:\n",
    "            result = result[result['subject_id'].isin(subject_ids)]\n",
    "        return result\n",
    "    \n",
    "    def get_subject_crossing_summary(self, subject_ids=None):\n",
    "        result = self.summary.copy()\n",
    "        if subject_ids is not None:\n",
    "            result = result[result['subject_id'].isin(subject_ids)]\n",
    "        return result\n",
    "\n",
    "# Set up mock quantile analyzer\n",
    "print(\"Setting up mock quantile analyzer...\")\n",
    "percentile_data = pd.DataFrame([\n",
    "    # sub001 - good overall performance\n",
    "    {\n",
    "        'subject_id': 'sub001', 'strata': 'Task1_ADVANCED_v3', 'is_current': True,\n",
    "        'trials_percentile': 92.0, 'errors_percentile': 88.0, 'accuracy_percentile': 95.0,\n",
    "        'trials_processed': 9.2, 'errors_processed': 8.8, 'accuracy_processed': 9.5,\n",
    "        'first_date': datetime.now() - timedelta(days=30), 'last_date': datetime.now()\n",
    "    },\n",
    "    # sub002 - poor overall performance\n",
    "    {\n",
    "        'subject_id': 'sub002', 'strata': 'Task1_INTERMEDIATE_v3', 'is_current': True,\n",
    "        'trials_percentile': 5.0, 'errors_percentile': 2.0, 'accuracy_percentile': 8.0,\n",
    "        'trials_processed': 0.5, 'errors_processed': 0.2, 'accuracy_processed': 0.8,\n",
    "        'first_date': datetime.now() - timedelta(days=30), 'last_date': datetime.now()\n",
    "    },\n",
    "    # sub003 - mixed performance\n",
    "    {\n",
    "        'subject_id': 'sub003', 'strata': 'Task1_BEGINNER_v3', 'is_current': True,\n",
    "        'trials_percentile': 50.0, 'errors_percentile': 75.0, 'accuracy_percentile': 25.0,\n",
    "        'trials_processed': 5.0, 'errors_processed': 7.5, 'accuracy_processed': 2.5,\n",
    "        'first_date': datetime.now() - timedelta(days=30), 'last_date': datetime.now()\n",
    "    },\n",
    "    # sub004 - only in quantile data\n",
    "    {\n",
    "        'subject_id': 'sub004', 'strata': 'Task2_ADVANCED_v3', 'is_current': True,\n",
    "        'trials_percentile': 99.0, 'errors_percentile': 98.0, 'accuracy_percentile': 97.0,\n",
    "        'trials_processed': 9.9, 'errors_processed': 9.8, 'accuracy_processed': 9.7,\n",
    "        'first_date': datetime.now() - timedelta(days=30), 'last_date': datetime.now()\n",
    "    }\n",
    "])\n",
    "\n",
    "class MockQuantileAnalyzer:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def create_comprehensive_dataframe(self, include_history=False):\n",
    "        return self.data.copy()\n",
    "\n",
    "# Attach mock analyzers to utils\n",
    "utils.threshold_analyzer = MockThresholdAnalyzer(threshold_data, threshold_summary)\n",
    "utils.quantile_analyzer = MockQuantileAnalyzer(percentile_data)\n",
    "\n",
    "# Override methods to match our mocks\n",
    "utils.get_threshold_crossings = utils.threshold_analyzer.get_threshold_crossings\n",
    "utils.get_subject_threshold_summary = utils.threshold_analyzer.get_subject_crossing_summary\n",
    "\n",
    "# Override validator to return True\n",
    "alert_service._validate_analyzers = lambda: True\n",
    "\n",
    "# Test the combined alert methods\n",
    "print(\"\\nTesting combined alert access methods...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined alerts retrieved for 4 subjects\n",
      "Subjects with both threshold and quantile alerts: ['sub002', 'sub001', 'sub003']\n",
      "\n",
      "Subjects with different alert criteria:\n",
      "Any alert: ['sub002', 'sub001', 'sub003', 'sub004']\n",
      "Threshold alerts (total_trials): ['sub002', 'sub001', 'sub003']\n",
      "Bad quantile alerts: ['sub002']\n",
      "\n",
      "Alert summaries:\n",
      "sub002: Threshold: 2 features with alerts: total_trials,error_count (2 total conditions) | Quantile: 1 Significantly Below Average: errors | 2 Below Average: trials, accuracy | in strata: Task1_INTERMEDIATE_v3\n",
      "sub001: Threshold: 2 features with alerts: total_trials,error_count (2 total conditions) | Quantile: 3 Above Average: trials, errors, accuracy | in strata: Task1_ADVANCED_v3\n",
      "sub003: Threshold: 2 features with alerts: total_trials,error_count (2 total conditions) | Quantile: 3 Average features | in strata: Task1_BEGINNER_v3\n",
      "sub004: Threshold: No threshold_alerts | Quantile: 1 Above Average: accuracy | 2 Significantly Above Average: trials, errors | in strata: Task2_ADVANCED_v3\n",
      "\n",
      "Critical alert check:\n",
      "sub002: Has critical alerts: True\n",
      "sub001: Has critical alerts: True\n",
      "sub003: Has critical alerts: True\n",
      "sub004: Has critical alerts: False\n",
      "\n",
      "Alert counts across all subjects:\n",
      "Total subjects with alerts: 4\n",
      "Subjects with threshold alerts: 3\n",
      "Subjects with quantile alerts: 4\n",
      "Subjects with critical alerts: 3\n",
      "Quantile category distribution: {'SB': 1, 'B': 2, 'N': 3, 'G': 4, 'SG': 2}\n"
     ]
    }
   ],
   "source": [
    "# Get combined alerts\n",
    "combined_alerts = alert_service.get_alerts()\n",
    "print(f\"\\nCombined alerts retrieved for {len(combined_alerts)} subjects\")\n",
    "\n",
    "# List which subjects have both types of alerts\n",
    "subjects_with_both = [\n",
    "    sid for sid, alerts in combined_alerts.items() \n",
    "    if alerts['threshold'] and alerts['quantile']['current']\n",
    "]\n",
    "print(f\"Subjects with both threshold and quantile alerts: {subjects_with_both}\")\n",
    "\n",
    "# Test subjects with alerts method\n",
    "print(\"\\nSubjects with different alert criteria:\")\n",
    "\n",
    "all_subjects = alert_service.get_subjects_with_alerts()\n",
    "print(f\"Any alert: {all_subjects}\")\n",
    "\n",
    "threshold_subjects = alert_service.get_subjects_with_alerts(threshold_features=['total_trials'])\n",
    "print(f\"Threshold alerts (total_trials): {threshold_subjects}\")\n",
    "\n",
    "bad_quantile_subjects = alert_service.get_subjects_with_alerts(\n",
    "    quantile_categories=['SB', 'B']\n",
    ")\n",
    "print(f\"Bad quantile alerts: {bad_quantile_subjects}\")\n",
    "\n",
    "# Test alert summaries\n",
    "print(\"\\nAlert summaries:\")\n",
    "for subject_id in combined_alerts:\n",
    "    summary = alert_service.get_alert_summary(subject_id)\n",
    "    print(f\"{subject_id}: {summary['combined']}\")\n",
    "\n",
    "# Test critical alerts\n",
    "print(\"\\nCritical alert check:\")\n",
    "for subject_id in combined_alerts:\n",
    "    has_critical = alert_service.has_critical_alerts(subject_id)\n",
    "    print(f\"{subject_id}: Has critical alerts: {has_critical}\")\n",
    "\n",
    "# Test alert counts\n",
    "alert_counts = alert_service.get_alert_counts()\n",
    "print(\"\\nAlert counts across all subjects:\")\n",
    "print(f\"Total subjects with alerts: {alert_counts['total_subjects_with_alerts']}\")\n",
    "print(f\"Subjects with threshold alerts: {alert_counts['threshold']['subjects_with_alerts']}\")\n",
    "print(f\"Subjects with quantile alerts: {alert_counts['quantile']['subjects_with_alerts']}\")\n",
    "print(f\"Subjects with critical alerts: {alert_counts['subjects_with_critical_alerts']}\")\n",
    "print(f\"Quantile category distribution: {alert_counts['quantile']['category_counts']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING APP UTILS INTEGRATION\n",
    "\n",
    "# Function to set up mock data for testing\n",
    "def setup_mock_data():\n",
    "    # Create sample session data\n",
    "    session_data = pd.DataFrame({\n",
    "        'subject_id': ['sub001', 'sub002', 'sub003'] * 2,\n",
    "        'session_date': [\n",
    "            datetime.now() - timedelta(days=5),\n",
    "            datetime.now() - timedelta(days=5),\n",
    "            datetime.now() - timedelta(days=5),\n",
    "            datetime.now() - timedelta(days=1),\n",
    "            datetime.now() - timedelta(days=1),\n",
    "            datetime.now() - timedelta(days=1)\n",
    "        ],\n",
    "        'total_trials': [90, 70, 100, 85, 65, 95],\n",
    "        'error_count': [5, 15, 8, 7, 18, 9],\n",
    "        'session': ['s1', 's1', 's1', 's2', 's2', 's2']\n",
    "    })\n",
    "    \n",
    "    # Create mock threshold data with crossing columns\n",
    "    threshold_data = session_data.copy()\n",
    "    threshold_data['total_trials_above_lower'] = threshold_data['total_trials'] >= 80\n",
    "    threshold_data['error_count_below_upper'] = threshold_data['error_count'] <= 10\n",
    "    \n",
    "    # Create mock percentile data\n",
    "    percentile_data = pd.DataFrame([\n",
    "        # sub001 - good performance\n",
    "        {\n",
    "            'subject_id': 'sub001', 'strata': 'Task1_ADVANCED_v3', 'is_current': True,\n",
    "            'trials_percentile': 92.0, 'errors_percentile': 88.0, 'accuracy_percentile': 95.0,\n",
    "            'first_date': datetime.now() - timedelta(days=30), 'last_date': datetime.now()\n",
    "        },\n",
    "        # sub002 - poor performance\n",
    "        {\n",
    "            'subject_id': 'sub002', 'strata': 'Task1_INTERMEDIATE_v3', 'is_current': True,\n",
    "            'trials_percentile': 5.0, 'errors_percentile': 2.0, 'accuracy_percentile': 8.0,\n",
    "            'first_date': datetime.now() - timedelta(days=30), 'last_date': datetime.now()\n",
    "        },\n",
    "        # sub003 - mixed performance\n",
    "        {\n",
    "            'subject_id': 'sub003', 'strata': 'Task1_BEGINNER_v3', 'is_current': True,\n",
    "            'trials_percentile': 50.0, 'errors_percentile': 75.0, 'accuracy_percentile': 25.0,\n",
    "            'first_date': datetime.now() - timedelta(days=30), 'last_date': datetime.now()\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    return session_data, threshold_data, percentile_data\n",
    "\n",
    "# Create a fresh AppUtils instance\n",
    "utils = AppUtils()\n",
    "\n",
    "# Set up mock data\n",
    "session_data, threshold_data, percentile_data = setup_mock_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock classes\n",
    "class MockDataLoader:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "\n",
    "class MockThresholdAnalyzer:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    # Use this method name for the test to match app_utils.py\n",
    "    def get_threshold_crossings(self, subject_ids=None, start_date=None, end_date=None):\n",
    "        result = self.data.copy()\n",
    "        if subject_ids is not None:\n",
    "            result = result[result['subject_id'].isin(subject_ids)]\n",
    "        return result\n",
    "    \n",
    "    def get_subject_crossing_summary(self, subject_ids=None):\n",
    "        # Create a simple summary from the data\n",
    "        summary_data = []\n",
    "        for subject_id in self.data['subject_id'].unique():\n",
    "            if subject_ids is not None and subject_id not in subject_ids:\n",
    "                continue\n",
    "                \n",
    "            subject_df = self.data[self.data['subject_id'] == subject_id]\n",
    "            \n",
    "            # Count how many sessions passed each threshold\n",
    "            trials_passed = subject_df['total_trials_above_lower'].sum()\n",
    "            errors_passed = subject_df['error_count_below_upper'].sum()\n",
    "            \n",
    "            summary_data.append({\n",
    "                'subject_id': subject_id,\n",
    "                'total_sessions': len(subject_df),\n",
    "                'total_trials_above_lower_count': trials_passed,\n",
    "                'total_trials_above_lower_percent': (trials_passed / len(subject_df)) * 100,\n",
    "                'error_count_below_upper_count': errors_passed,\n",
    "                'error_count_below_upper_percent': (errors_passed / len(subject_df)) * 100\n",
    "            })\n",
    "            \n",
    "        return pd.DataFrame(summary_data)\n",
    "\n",
    "class MockQuantileAnalyzer:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def create_comprehensive_dataframe(self, include_history=False):\n",
    "        return self.data.copy()\n",
    "\n",
    "# Set up the AppUtils instance with our mocks\n",
    "utils.data_loader = MockDataLoader(session_data)\n",
    "utils.threshold_analyzer = MockThresholdAnalyzer(threshold_data)\n",
    "utils.quantile_analyzer = MockQuantileAnalyzer(percentile_data)\n",
    "\n",
    "# Add mock methods that match EXACTLY what AlertService expects\n",
    "# Note the singular \"crossing\" to match your app_utils.py method\n",
    "utils.get_threshold_crossing = utils.threshold_analyzer.get_threshold_crossings\n",
    "utils.get_subject_threshold_summary = utils.threshold_analyzer.get_subject_crossing_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing AppUtils integration with AlertService...\n",
      "Alert service initialized: True\n",
      "\n",
      "Got alerts for 3 subjects through AppUtils\n",
      "\n",
      "Subject sub003:\n",
      "  Has critical alerts: True\n",
      "  Alert summary: Threshold: 2 features with alerts: total_trials,error_count (2 total conditions) | Quantile: 3 Average features | in strata: Task1_BEGINNER_v3\n",
      "\n",
      "Subject sub001:\n",
      "  Has critical alerts: True\n",
      "  Alert summary: Threshold: 2 features with alerts: total_trials,error_count (2 total conditions) | Quantile: 3 Above Average: trials, errors, accuracy | in strata: Task1_ADVANCED_v3\n",
      "\n",
      "Subject sub002:\n",
      "  Has critical alerts: True\n",
      "  Alert summary: Threshold: No threshold_alerts | Quantile: 1 Significantly Below Average: errors | 2 Below Average: trials, accuracy | in strata: Task1_INTERMEDIATE_v3\n",
      "\n",
      "Subjects with bad performance: ['sub002']\n",
      "\n",
      "Alert counts:\n",
      "Total subjects with alerts: 3\n",
      "Subjects with critical alerts: 3\n"
     ]
    }
   ],
   "source": [
    "# Test the integrated alert service\n",
    "print(\"Testing AppUtils integration with AlertService...\")\n",
    "\n",
    "# Initialize the alert service through AppUtils\n",
    "alert_service = utils.initialize_alert_service()\n",
    "print(f\"Alert service initialized: {alert_service is not None}\")\n",
    "\n",
    "# Get alerts through AppUtils\n",
    "alerts = utils.get_alerts()\n",
    "print(f\"\\nGot alerts for {len(alerts)} subjects through AppUtils\")\n",
    "\n",
    "# Check if subjects have alerts\n",
    "for subject_id in alerts:\n",
    "    has_critical = utils.has_critical_alerts(subject_id)\n",
    "    summary = utils.get_alert_summary(subject_id)\n",
    "    print(f\"\\nSubject {subject_id}:\")\n",
    "    print(f\"  Has critical alerts: {has_critical}\")\n",
    "    print(f\"  Alert summary: {summary['combined']}\")\n",
    "\n",
    "# Get subjects with specific alert criteria\n",
    "bad_performers = utils.get_subjects_with_alerts(\n",
    "    quantile_categories=['SB', 'B']\n",
    ")\n",
    "print(f\"\\nSubjects with bad performance: {bad_performers}\")\n",
    "\n",
    "# Get alert counts\n",
    "alert_counts = utils.get_alert_counts()\n",
    "print(\"\\nAlert counts:\")\n",
    "print(f\"Total subjects with alerts: {alert_counts['total_subjects_with_alerts']}\")\n",
    "print(f\"Subjects with critical alerts: {alert_counts['subjects_with_critical_alerts']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
